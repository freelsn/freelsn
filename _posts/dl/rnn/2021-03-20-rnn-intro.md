---
layout: post
title: RNN 简介
---

# 应用场景

当要处理的输入或输出是 sequence 时，比如下图。

![rnn applications](/assets/images/rnn/applications.png)

# 举例

输入一段话，标记出人名。

![motivating example](/assets/images/rnn/motivation.png)

这句话共有 9 个 word，其实也可以把句号算作 1 个 word，同时在最后增加 <EOS> (end of sentence) 标识符。这样，这句话就有 11 个 word。其中

- $$x^(<1>)$$ 代表输入序列的第 i 个 item
- $x^(i)<t>$ 表示这是整个 training data 里的第 i 个 example 中第 t 个 word
- $T_x^(i)$ 表示第 i 个 sample 的长度，这里是 9

对 y 同理。

# 表示 word

根据具体问题，创建 vocabulary，其中包括所有的 word，每个 word 用 one-hot 表示。

![representing words](/assets/images/rnn/representing-words.png)

# 为什么 FC 或 CNN 不行

- 输入或输出的长度不固定。即使可以按照最长的 sequence 做 padding，但缺点是浪费计算资源
- word 出现的位置是不固定的，RNN 能学到 word 和上下文的关系

![why-not-a-standard-network](/assets/images/rnn/why-not-a-standard-network.png)

# RNN 结构

![rnn](/assets/images/rnn/structure.png)

所有的 w 和 b 都是共享的。

$$\hat{y}^(<1>)=w_(aa)\cdot a^(<0>)

![rnn types](/assets/images/rnn/rnn-types.png)

# Language Modelling

判断某个 sequence 出现的概率。

比如在语音识别中，翻译一段发音一样句子。

![language-modelling-speech-recognition](/assets/images/rnn/language-modelling-speech-recognition.png)

如果用 RNN 实现，做法是输入一个或多个 word，预测下一个 word。

![language-modelling-rnn](/assets/images/rnn/language-modelling-rnn.png)

第一个输入 $$x^(<1>)$$ 是假的，一般是 0 向量，输出的 $$\hat{y}^(<1>)$$ 要去拟合 `Cats` 的 one-hot。第二个输入则是 `Cats`，输出要拟合 `average`，从而模拟 Cats 后面出现 average 的情况。

## Sampling a Sequence

这样训练出来的 RNN 可以应用到 sequence sampling。输入 0 向量，从输出中选择概率最大的 word，将其作为下一个输入，得到下一个输出。依此类推，知道输出 <EOS> 或达到指定输出个数。这样所有的输出就是一段由 RNN 生成的段落

![sequence-sampling](/assets/images/rnn/sequence-sampling.png)

## Character-level Language Model

上面提到的 modelling 方法是以 word 为最小单位，其实还可以以 character 为最小单位。这样 vocabulary 是有限的，包括基本字母，空格，标点符号和特殊字符。而且不会出现 <UNK> unknown item。缺点就是 sequence 太长。不过计算能力强的电脑还是可以训练这种模型的。
